{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DiTo97/binarization-segformer/blob/main/fine-tuning.ipynb)\n",
        "\n",
        "# Fine-tuning Segformer for Document Image Binarization\n",
        "\n",
        "A notebook by F. Minutoli ([@DiTo97](https://github.com/DiTo97)) that fine-tunes a Segformer model for document image binarization"
      ],
      "metadata": {
        "id": "kHhmCKkNspko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "requirements = \" \".join([\n",
        "    \"accelerate==0.18.0\",\n",
        "    \"albumentations==1.3.0\",\n",
        "    \"datasets==2.11.0\",\n",
        "    \"evaluate==0.4.0\",\n",
        "    \"huggingface-hub==0.13.4\",\n",
        "    \"transformers==4.27.4\"\n",
        "])\n",
        "\n",
        "!python -m pip install --upgrade pip\n",
        "!python -m pip install $requirements"
      ],
      "metadata": {
        "id": "MM5vjXhqzxKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login"
      ],
      "metadata": {
        "id": "Kc8nRW5-PXZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_login()"
      ],
      "metadata": {
        "id": "n97yPqpGPZS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Dataset"
      ],
      "metadata": {
        "id": "4NfSYzsksymV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Leedeng/SauvolaNet.git"
      ],
      "metadata": {
        "id": "1lJUa-B2YrFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import sys\n",
        "import typing\n",
        "from typing import Any\n",
        "\n",
        "import datasets\n",
        "import numpy as np\n",
        "import numpy.typing as nptyping\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "A0yHI8-VxWAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(bitmap: Image.Image) -> Image.Image:\n",
        "    bitmap = bitmap.convert(\"L\")\n",
        "    bitmap = np.array(bitmap).astype(np.uint8)\n",
        "    condition = bitmap < np.max(bitmap)\n",
        "    bitmap = np.where(condition, 1, 0).astype(np.bool_)\n",
        "    bitmap = Image.fromarray(bitmap)\n",
        "\n",
        "    return bitmap"
      ],
      "metadata": {
        "id": "ieXXmK7b_M4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_examples(\n",
        "    batch: typing.Dict[str, typing.List[Any]]\n",
        ") -> typing.Dict[str, typing.List[Any]]:\n",
        "    \"\"\"It prepares a batch of examples for semantic segmentation\"\"\"\n",
        "    sources = batch[\"source\"]\n",
        "    targets = batch[\"target\"]\n",
        "\n",
        "    batch = {\n",
        "        \"labelmap\": [normalize(Image.open(tgt)) for tgt in targets],\n",
        "        \"pixelmap\": [Image.open(src) for src in sources]\n",
        "    }\n",
        "\n",
        "    return batch"
      ],
      "metadata": {
        "id": "8J0xuvfh6xZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sauvolanet_src = \"SauvolaNet/SauvolaDocBin\"\n",
        "sauvolanet_dataset = \"SauvolaNet/Dataset\"\n",
        "\n",
        "sys.path.insert(0, sauvolanet_src)\n",
        "from dataUtils import collect_binarization_by_dataset\n",
        "\n",
        "collection = collect_binarization_by_dataset(sauvolanet_dataset)\n",
        "\n",
        "sys.path.remove(sauvolanet_src)\n",
        "\n",
        "del sauvolanet_src\n",
        "del sauvolanet_dataset\n",
        "del collect_binarization_by_dataset\n",
        "\n",
        "features = datasets.Features({\n",
        "    \"ensemble\": datasets.Value(\"string\"),\n",
        "    \"source\": datasets.Value(\"string\"),\n",
        "    \"target\": datasets.Value(\"string\"),\n",
        "})\n",
        "\n",
        "for name, examples in tqdm(collection.items(), desc=\"Loading datasets\"):\n",
        "    sources, targets = zip(*examples)\n",
        "\n",
        "    sources = sorted(sources)\n",
        "    targets = sorted(targets)\n",
        "\n",
        "    dataset = {\"source\": sources, \"target\": targets, \"ensemble\": [name] * len(sources)}\n",
        "    dataset = datasets.Dataset.from_dict(dataset, features)\n",
        "\n",
        "    collection[name] = dataset\n",
        "\n",
        "collection = datasets.concatenate_datasets([\n",
        "    dataset for _, dataset in collection.items()\n",
        "])\n",
        "\n",
        "features = datasets.Features({\n",
        "    \"ensemble\": datasets.Value(\"string\"),\n",
        "    \"labelmap\": datasets.Image(),\n",
        "    \"pixelmap\": datasets.Image(),\n",
        "})\n",
        "\n",
        "collection = collection.map(\n",
        "    prepare_examples, \n",
        "    batched=True,\n",
        "    features=features, \n",
        "    remove_columns=[\"source\", \"target\"]\n",
        ")\n",
        "\n",
        "collection = collection.class_encode_column(\"ensemble\")\n",
        "\n",
        "del features\n",
        "\n",
        "collection = collection.train_test_split(\n",
        "    seed=10,\n",
        "    shuffle=True,\n",
        "    stratify_by_column=\"ensemble\",\n",
        "    train_size=0.75\n",
        ")\n",
        "\n",
        "train_dataset = collection[\"train\"]\n",
        "test_dataset  = collection[ \"test\"]\n",
        "\n",
        "del collection"
      ],
      "metadata": {
        "id": "Dj2-dnCBvhVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [\"background\", \"text\"]\n",
        "num_labels = len(labels)\n",
        "\n",
        "id2label = {key: val for key, val in enumerate(labels)}\n",
        "label2id = {val: key for key, val in enumerate(labels)}\n",
        "\n",
        "del labels"
      ],
      "metadata": {
        "id": "oTHsmOTaMsOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Augmentation"
      ],
      "metadata": {
        "id": "3YsmKoUMNRtf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import albumentations\n",
        "import cv2\n",
        "import transformers\n",
        "from transformers import set_seed"
      ],
      "metadata": {
        "id": "SnfP2XpoOHlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(10)"
      ],
      "metadata": {
        "id": "9nVPfUEUOVvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processor = transformers.SegformerImageProcessor()\n",
        "\n",
        "FLAGS = {\n",
        "    # The general kwargs\n",
        "    \"border_mode\": cv2.BORDER_CONSTANT,\n",
        "    \"fill_value\": 255,\n",
        "    \"mask_fill_value\": 0,\n",
        "    \"proba\": 0.1,\n",
        "\n",
        "    # The color kwargs\n",
        "    \"brightness\": 0.25, \n",
        "    \"contrast\": 0.25, \n",
        "    \"saturation\": 0.25, \n",
        "    \"hue\": 0.1,\n",
        "    \n",
        "    # The crop kwargs\n",
        "    \"min_height\": processor.size[\"height\"],\n",
        "    \"min_width\" : processor.size[ \"width\"],\n",
        "    \n",
        "    # The geometric kwargs\n",
        "    \"rotate\": (-90, 90),\n",
        "    \"translate_percent\": 0.1\n",
        "}\n",
        "\n",
        "transform1 = albumentations.Compose([\n",
        "    albumentations.ColorJitter(\n",
        "        brightness=FLAGS[\"brightness\"], \n",
        "        contrast=FLAGS[\"contrast\"], \n",
        "        saturation=FLAGS[\"saturation\"], \n",
        "        hue=FLAGS[\"hue\"]\n",
        "    )\n",
        "])\n",
        "\n",
        "transform2 = albumentations.Compose([\n",
        "    albumentations.Flip(p=FLAGS[\"proba\"]),\n",
        "    albumentations.Affine(\n",
        "        p=FLAGS[\"proba\"],\n",
        "        cval=FLAGS[\"fill_value\"],\n",
        "        cval_mask=FLAGS[\"mask_fill_value\"],\n",
        "        mode=FLAGS[\"border_mode\"],\n",
        "        rotate=FLAGS[\"rotate\"], \n",
        "        translate_percent=FLAGS[\"translate_percent\"],\n",
        "    ),\n",
        "    albumentations.PadIfNeeded(\n",
        "        border_mode=FLAGS[\"border_mode\"],\n",
        "        mask_value=FLAGS[\"mask_fill_value\"],\n",
        "        min_height=FLAGS[\"min_height\"], \n",
        "        min_width=FLAGS[\"min_width\"], \n",
        "        value=FLAGS[\"fill_value\"],\n",
        "    ),\n",
        "    albumentations.RandomCrop(\n",
        "        p=FLAGS[\"proba\"],\n",
        "        height=FLAGS[\"min_height\"], \n",
        "        width=FLAGS[\"min_width\"],\n",
        "    )\n",
        "])\n",
        "\n",
        "def train_transform(\n",
        "    batch: typing.Dict[str, typing.List[Any]]\n",
        ") -> transformers.BatchFeature:\n",
        "    images = [image.convert(\"RGB\") for image in batch[\"pixelmap\"]]\n",
        "    images = [np.array(image) for image in images]\n",
        "    images = [transform1(image=image)[\"image\"] for image in images]\n",
        "\n",
        "    labels = [np.array(label).astype(np.uint8) for label in batch[\"labelmap\"]]\n",
        "\n",
        "    examples = [\n",
        "        transform2(image=image, mask=mask) for image, mask in zip(images, labels)\n",
        "    ]\n",
        "\n",
        "    images = [example[\"image\"] for example in examples]\n",
        "    labels = [example[ \"mask\"] for example in examples]\n",
        "\n",
        "    encoding = processor(images, labels)\n",
        "    return encoding\n",
        "\n",
        "def  test_transform(\n",
        "    batch: typing.Dict[str, typing.List[Any]]\n",
        ") -> transformers.BatchFeature:\n",
        "    images = [image.convert(\"RGB\") for image in batch[\"pixelmap\"]]\n",
        "    labels = [label for label in batch[\"labelmap\"]]\n",
        "\n",
        "    encoding = processor(images, labels)\n",
        "    return encoding\n",
        "\n",
        "train_dataset.set_transform(train_transform)\n",
        "test_dataset.set_transform(test_transform)"
      ],
      "metadata": {
        "id": "xBhurCGKF5Rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Training"
      ],
      "metadata": {
        "id": "vi8OMuCCNgXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers.trainer_utils import get_last_checkpoint"
      ],
      "metadata": {
        "id": "QAuL5fYuLQe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cuda = torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "VGEqQsYa01TE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets.logging.set_verbosity_error()\n",
        "evaluate.logging.set_verbosity_error()\n",
        "transformers.logging.set_verbosity_error()"
      ],
      "metadata": {
        "id": "QCG0Ndk-7KTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FLAGS = {\n",
        "    \"accumulation_steps\": 4,\n",
        "    \"base_model_name\": \"nvidia/mit-b0\",\n",
        "    \"batch_size\": 4,\n",
        "    \"fp16\": cuda,\n",
        "    \"learning_rate\": 5e-5,\n",
        "    \"metric\": \"mean_iou\",\n",
        "    \"model_name\": \"segformer-b0-for-binarization\",\n",
        "    \"num_epochs\": 50,\n",
        "    \"optimizer\": \"adamw_torch\",\n",
        "    \"scheduler_type\": \"cosine\"\n",
        "}"
      ],
      "metadata": {
        "id": "KB73Q2G7J0dD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processor.push_to_hub(FLAGS[\"model_name\"])"
      ],
      "metadata": {
        "id": "kdogl_mM1JdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger = transformers.logging.get_logger()\n",
        "metric = evaluate.load(FLAGS[\"metric\"])\n",
        "\n",
        "model_kwargs = {\n",
        "    \"id2label\": id2label, \n",
        "    \"label2id\": label2id\n",
        "}\n",
        "\n",
        "model = transformers.SegformerForSemanticSegmentation.from_pretrained(\n",
        "    FLAGS[\"base_model_name\"], **model_kwargs\n",
        ")\n",
        "\n",
        "\n",
        "def compute_metrics(outputs: transformers.EvalPrediction) -> typing.Dict[str, float]:\n",
        "    with torch.no_grad():\n",
        "        logits, labels = outputs\n",
        "        logits = torch.from_numpy(logits)\n",
        "\n",
        "        # It upscales the logits to the size of the label\n",
        "        logits = nn.functional.interpolate(\n",
        "            logits,\n",
        "            size=labels.shape[-2:],\n",
        "            mode=\"bilinear\",\n",
        "            align_corners=False,\n",
        "        ).argmax(dim=1)\n",
        "\n",
        "        predictions = logits.detach().cpu().numpy()\n",
        "\n",
        "        # FIXME: For more information, see\n",
        "        # https://github.com/huggingface/evaluate/pull/328#issuecomment-1286866576\n",
        "        metrics = metric._compute(\n",
        "                predictions=predictions,\n",
        "                references=labels,\n",
        "                num_labels=num_labels,\n",
        "                ignore_index=0,  # The background info is ignored\n",
        "                reduce_labels=processor.do_reduce_labels,\n",
        "            )\n",
        "        \n",
        "        # It adds per-category metrics as separate key-val pairs\n",
        "        per_category_accuracy = metrics.pop(\"per_category_accuracy\").tolist()\n",
        "        per_category_iou = metrics.pop(\"per_category_iou\").tolist()\n",
        "\n",
        "        metrics.update({f\"accuracy_{id2label[key]}\": val for key, val in enumerate(per_category_accuracy)})\n",
        "        metrics.update({f\"iou_{id2label[key]}\": val for key, val in enumerate(per_category_iou)})\n",
        "        \n",
        "        return metrics\n",
        "\n",
        "\n",
        "training_args = transformers.TrainingArguments(\n",
        "    auto_find_batch_size=True,\n",
        "    eval_accumulation_steps=FLAGS[\"accumulation_steps\"],\n",
        "    eval_steps=10,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    fp16=FLAGS[\"fp16\"],\n",
        "    full_determinism=True,\n",
        "    gradient_accumulation_steps=FLAGS[\"accumulation_steps\"],\n",
        "    hub_model_id=FLAGS[\"model_name\"],\n",
        "    hub_strategy=\"end\",\n",
        "    learning_rate=FLAGS[\"learning_rate\"],\n",
        "    load_best_model_at_end=True,\n",
        "    logging_steps=5,\n",
        "    lr_scheduler_type=FLAGS[\"scheduler_type\"],\n",
        "    num_train_epochs=FLAGS[\"num_epochs\"],\n",
        "    optim=FLAGS[\"optimizer\"],\n",
        "    output_dir=FLAGS[\"model_name\"],\n",
        "    per_device_eval_batch_size=FLAGS[\"batch_size\"],\n",
        "    per_device_train_batch_size=FLAGS[\"batch_size\"],\n",
        "    push_to_hub=True,\n",
        "    remove_unused_columns=False,  # https://discuss.huggingface.co/t/divide-by-zero-error-when-following-ch7-tutorial/18393/6\n",
        "    report_to=\"tensorboard\",\n",
        "    save_steps=10,\n",
        "    save_strategy=\"steps\",\n",
        "    save_total_limit=3,\n",
        "    seed=10,\n",
        "    warmup_steps=10,\n",
        ")\n",
        "\n",
        "callbacks = [\n",
        "    transformers.EarlyStoppingCallback(early_stopping_patience=5)\n",
        "]\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    args=training_args,\n",
        "    callbacks=callbacks,\n",
        "    compute_metrics=compute_metrics,\n",
        "    eval_dataset=test_dataset,\n",
        "    model=model,    \n",
        "    train_dataset=train_dataset\n",
        ")\n",
        "\n",
        "try:\n",
        "    checkpoint = get_last_checkpoint(FLAGS[\"model_name\"])\n",
        "except FileNotFoundError:\n",
        "    logger.debug(\"No checkpoint\")\n",
        "    checkpoint = None\n",
        "\n",
        "resume_from_checkpoint = checkpoint is not None\n",
        "\n",
        "trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
        "\n",
        "kwargs = {\n",
        "    \"finetuned_from\": FLAGS[\"base_model_name\"],\n",
        "    \"tags\": [\n",
        "        \"computer-vision\",\n",
        "        \"document-image-binarization\"\n",
        "        \"image-segmentation\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "trainer.push_to_hub(**kwargs)"
      ],
      "metadata": {
        "id": "8foaMGZlLEzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Inference\n",
        "\n",
        "For a complete example, see T. Cornille's official Segformer [blog post](https://huggingface.co/blog/fine-tune-segformer)"
      ],
      "metadata": {
        "id": "w2RW-caNNh4T"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nTgfBrTw25u1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}