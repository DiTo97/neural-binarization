{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DiTo97/binarization-segformer/blob/main/fine-tuning.ipynb)\n",
        "\n",
        "# Fine-tuning Segformer for Document Image Binarization\n",
        "\n",
        "A notebook by F. Minutoli ([@DiTo97](https://github.com/DiTo97)) that fine-tunes a Segformer model for document image binarization"
      ],
      "metadata": {
        "id": "kHhmCKkNspko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "requirements = \" \".join([\n",
        "    \"accelerate==0.18.0\",\n",
        "    \"albumentations==1.3.0\",\n",
        "    \"bitsandbytes==0.38.1\",\n",
        "    \"datasets==2.11.0\",\n",
        "    \"evaluate==0.4.0\",\n",
        "    \"huggingface-hub==0.13.4\",\n",
        "    \"transformers==4.27.4\"\n",
        "])\n",
        "\n",
        "!python -m pip install --upgrade pip\n",
        "!python -m pip install $requirements"
      ],
      "metadata": {
        "id": "MM5vjXhqzxKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login"
      ],
      "metadata": {
        "id": "Kc8nRW5-PXZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_login()"
      ],
      "metadata": {
        "id": "n97yPqpGPZS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Dataset"
      ],
      "metadata": {
        "id": "4NfSYzsksymV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Leedeng/SauvolaNet.git"
      ],
      "metadata": {
        "id": "1lJUa-B2YrFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import sys\n",
        "import typing\n",
        "from typing import Any\n",
        "\n",
        "import datasets\n",
        "import numpy as np\n",
        "import numpy.typing as nptyping\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "A0yHI8-VxWAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(bitmap: Image.Image) -> Image.Image:\n",
        "    bitmap = bitmap.convert(\"L\")\n",
        "    bitmap = np.array(bitmap).astype(np.uint8)\n",
        "    condition = bitmap < np.max(bitmap)\n",
        "    bitmap = np.where(condition, 1, 0).astype(np.bool_)\n",
        "    bitmap = Image.fromarray(bitmap)\n",
        "\n",
        "    return bitmap"
      ],
      "metadata": {
        "id": "ieXXmK7b_M4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_examples(\n",
        "    batch: typing.Dict[str, typing.List[Any]]\n",
        ") -> typing.Dict[str, typing.List[Any]]:\n",
        "    \"\"\"It prepares a batch of examples for semantic segmentation\"\"\"\n",
        "    sources = batch[\"source\"]\n",
        "    targets = batch[\"target\"]\n",
        "\n",
        "    batch = {\n",
        "        \"labelmap\": [normalize(Image.open(tgt)) for tgt in targets],\n",
        "        \"pixelmap\": [Image.open(src) for src in sources]\n",
        "    }\n",
        "\n",
        "    return batch"
      ],
      "metadata": {
        "id": "8J0xuvfh6xZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sauvolanet_src = \"SauvolaNet/SauvolaDocBin\"\n",
        "sauvolanet_dataset = \"SauvolaNet/Dataset\"\n",
        "\n",
        "sys.path.insert(0, sauvolanet_src)\n",
        "from dataUtils import collect_binarization_by_dataset\n",
        "\n",
        "collection = collect_binarization_by_dataset(sauvolanet_dataset)\n",
        "\n",
        "sys.path.remove(sauvolanet_src)\n",
        "\n",
        "del sauvolanet_src\n",
        "del sauvolanet_dataset\n",
        "del collect_binarization_by_dataset\n",
        "\n",
        "features = datasets.Features({\n",
        "    \"ensemble\": datasets.Value(\"string\"),\n",
        "    \"source\": datasets.Value(\"string\"),\n",
        "    \"target\": datasets.Value(\"string\"),\n",
        "})\n",
        "\n",
        "for name, examples in tqdm(collection.items(), desc=\"Loading datasets\"):\n",
        "    sources, targets = zip(*examples)\n",
        "\n",
        "    sources = sorted(sources)\n",
        "    targets = sorted(targets)\n",
        "\n",
        "    dataset = {\"source\": sources, \"target\": targets, \"ensemble\": [name] * len(sources)}\n",
        "    dataset = datasets.Dataset.from_dict(dataset, features)\n",
        "\n",
        "    collection[name] = dataset\n",
        "\n",
        "collection = datasets.concatenate_datasets([\n",
        "    dataset for _, dataset in collection.items()\n",
        "])\n",
        "\n",
        "features = datasets.Features({\n",
        "    \"ensemble\": datasets.Value(\"string\"),\n",
        "    \"labelmap\": datasets.Image(),\n",
        "    \"pixelmap\": datasets.Image(),\n",
        "})\n",
        "\n",
        "collection = collection.map(\n",
        "    prepare_examples, \n",
        "    batched=True,\n",
        "    features=features, \n",
        "    remove_columns=[\"source\", \"target\"]\n",
        ")\n",
        "\n",
        "collection = collection.class_encode_column(\"ensemble\")\n",
        "\n",
        "del features\n",
        "\n",
        "collection = collection.train_test_split(\n",
        "    seed=10,\n",
        "    shuffle=True,\n",
        "    stratify_by_column=\"ensemble\",\n",
        "    train_size=0.75\n",
        ")\n",
        "\n",
        "train_dataset = collection[\"train\"]\n",
        "test_dataset  = collection[ \"test\"]\n",
        "\n",
        "del collection"
      ],
      "metadata": {
        "id": "Dj2-dnCBvhVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [\"background\", \"text\"]\n",
        "num_labels = len(labels)\n",
        "\n",
        "id2label = {key: val for key, val in enumerate(labels)}\n",
        "label2id = {val: key for key, val in enumerate(labels)}\n",
        "\n",
        "del labels"
      ],
      "metadata": {
        "id": "oTHsmOTaMsOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Augmentation"
      ],
      "metadata": {
        "id": "3YsmKoUMNRtf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import albumentations\n",
        "import cv2\n",
        "import transformers\n",
        "from transformers import set_seed"
      ],
      "metadata": {
        "id": "SnfP2XpoOHlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(10)"
      ],
      "metadata": {
        "id": "9nVPfUEUOVvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model_name = \"nvidia/segformer-b3-finetuned-cityscapes-1024-1024\"\n",
        "base_model_size = {\"height\": 640, \"width\": 640}\n",
        "\n",
        "processor = transformers.SegformerImageProcessor.from_pretrained(base_model_name)\n",
        "processor.size.update(base_model_size)"
      ],
      "metadata": {
        "id": "FuGbJ6xz6n9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FLAGS = {\n",
        "    # The general kwargs\n",
        "    \"border_mode\": cv2.BORDER_CONSTANT,\n",
        "    \"fill_value\": 255,\n",
        "    \"mask_fill_value\": 0,\n",
        "    \"proba\": 0.1,\n",
        "\n",
        "    # The color kwargs\n",
        "    \"brightness\": 0.25, \n",
        "    \"contrast\": 0.25, \n",
        "    \"saturation\": 0.25, \n",
        "    \"hue\": 0.1,\n",
        "    \n",
        "    # The crop kwargs\n",
        "    \"min_height\": processor.size[\"height\"],\n",
        "    \"min_width\" : processor.size[ \"width\"],\n",
        "    \n",
        "    # The geometric kwargs\n",
        "    \"rotate\": (-90, 90),\n",
        "    \"translate_percent\": 0.1\n",
        "}\n",
        "\n",
        "transform1 = albumentations.Compose([\n",
        "    albumentations.ColorJitter(\n",
        "        brightness=FLAGS[\"brightness\"], \n",
        "        contrast=FLAGS[\"contrast\"], \n",
        "        saturation=FLAGS[\"saturation\"], \n",
        "        hue=FLAGS[\"hue\"]\n",
        "    )\n",
        "])\n",
        "\n",
        "transform2 = albumentations.Compose([\n",
        "    albumentations.Flip(p=FLAGS[\"proba\"]),\n",
        "    albumentations.Affine(\n",
        "        p=FLAGS[\"proba\"],\n",
        "        cval=FLAGS[\"fill_value\"],\n",
        "        cval_mask=FLAGS[\"mask_fill_value\"],\n",
        "        mode=FLAGS[\"border_mode\"],\n",
        "        rotate=FLAGS[\"rotate\"], \n",
        "        translate_percent=FLAGS[\"translate_percent\"],\n",
        "    ),\n",
        "    albumentations.PadIfNeeded(\n",
        "        border_mode=FLAGS[\"border_mode\"],\n",
        "        mask_value=FLAGS[\"mask_fill_value\"],\n",
        "        min_height=FLAGS[\"min_height\"], \n",
        "        min_width=FLAGS[\"min_width\"], \n",
        "        value=FLAGS[\"fill_value\"],\n",
        "    ),\n",
        "    albumentations.RandomCrop(\n",
        "        p=FLAGS[\"proba\"],\n",
        "        height=FLAGS[\"min_height\"], \n",
        "        width=FLAGS[\"min_width\"],\n",
        "    )\n",
        "])\n",
        "\n",
        "def train_transform(\n",
        "    batch: typing.Dict[str, typing.List[Any]]\n",
        ") -> transformers.BatchFeature:\n",
        "    images = [image.convert(\"RGB\") for image in batch[\"pixelmap\"]]\n",
        "    images = [np.array(image) for image in images]\n",
        "    images = [transform1(image=image)[\"image\"] for image in images]\n",
        "\n",
        "    labels = [np.array(label).astype(np.uint8) for label in batch[\"labelmap\"]]\n",
        "\n",
        "    examples = [\n",
        "        transform2(image=image, mask=mask) for image, mask in zip(images, labels)\n",
        "    ]\n",
        "\n",
        "    images = [example[\"image\"] for example in examples]\n",
        "    labels = [example[ \"mask\"] for example in examples]\n",
        "\n",
        "    encoding = processor(images, labels)\n",
        "    return encoding\n",
        "\n",
        "def  test_transform(\n",
        "    batch: typing.Dict[str, typing.List[Any]]\n",
        ") -> transformers.BatchFeature:\n",
        "    images = [image.convert(\"RGB\") for image in batch[\"pixelmap\"]]\n",
        "    labels = [label for label in batch[\"labelmap\"]]\n",
        "\n",
        "    encoding = processor(images, labels)\n",
        "    return encoding"
      ],
      "metadata": {
        "id": "xBhurCGKF5Rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.set_transform(train_transform)\n",
        "test_dataset.set_transform(test_transform)"
      ],
      "metadata": {
        "id": "u3EHJJcZ7seI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Training"
      ],
      "metadata": {
        "id": "vi8OMuCCNgXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import evaluate\n",
        "import bitsandbytes\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers.trainer_utils import get_last_checkpoint\n",
        "from transformers.trainer_pt_utils import get_parameter_names"
      ],
      "metadata": {
        "id": "QAuL5fYuLQe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cuda = torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "VGEqQsYa01TE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets.logging.set_verbosity_error()\n",
        "# evaluate.logging.set_verbosity_error()\n",
        "transformers.logging.set_verbosity_error()"
      ],
      "metadata": {
        "id": "QCG0Ndk-7KTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FLAGS = {\n",
        "    \"accumulation_steps\": 4,\n",
        "    \"base_model_name\": base_model_name,\n",
        "    \"batch_size\": 4,\n",
        "    \"fp16\": cuda,\n",
        "    \"learning_rate\": 5e-5,\n",
        "    \"metric\": \"dibco\",\n",
        "    \"model_name\": \"binarization-segformer-b3\",\n",
        "    \"num_epochs\": 50,\n",
        "    \"optimizer\": \"adamw_torch\",\n",
        "    \"scheduler_type\": \"cosine\"\n",
        "}"
      ],
      "metadata": {
        "id": "KB73Q2G7J0dD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processor.push_to_hub(FLAGS[\"model_name\"])"
      ],
      "metadata": {
        "id": "kdogl_mM1JdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue May 21 21:02:46 2019\n",
        "\n",
        "@author: VIPlab\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import cv2\n",
        "import math\n",
        "from scipy import ndimage as ndi\n",
        "\n",
        "#predict_img = 'E:\\Document-Binarization\\DIBCO_metrics\\DIBCO_metrics\\P03_adotsu.tif'\n",
        "#predict_img = cv2.imread(predict_img, 0)\n",
        "#GT_img = 'E:\\Document-Binarization\\DIBCO_metrics\\DIBCO_metrics\\P03_GT.tif'\n",
        "#GT_img = cv2.imread(GT_img, 0)\n",
        "#predict_img_ = np.copy(predict_img)\n",
        "#predict_img_ = predict_img_/255\n",
        "#GT_img_ = np.copy(GT_img)\n",
        "#GT_img_ = GT_img_/255\n",
        "\n",
        "\n",
        "G123_LUT = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,\n",
        "       0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "       0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
        "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n",
        "       1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
        "       0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "       0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
        "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,\n",
        "       0, 0, 0], dtype=bool)\n",
        "\n",
        "G123P_LUT = np.array([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
        "       0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
        "       1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "       0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,\n",
        "       0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
        "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
        "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
        "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "       0, 0, 0], dtype=bool)\n",
        "\n",
        "def bwmorph_thin(image, n_iter=None):\n",
        "    # check parameters\n",
        "    if n_iter is None:\n",
        "        n = -1\n",
        "    elif n_iter <= 0:\n",
        "        raise ValueError('n_iter must be > 0')\n",
        "    else:\n",
        "        n = n_iter\n",
        "    \n",
        "    # check that we have a 2d binary image, and convert it\n",
        "    # to uint8\n",
        "    skel = np.array(image).astype(np.uint8)\n",
        "    \n",
        "    if skel.ndim != 2:\n",
        "        raise ValueError('2D array required')\n",
        "    if not np.all(np.in1d(image.flat,(0,1))):\n",
        "        raise ValueError('Image contains values other than 0 and 1')\n",
        "\n",
        "    # neighborhood mask\n",
        "    mask = np.array([[ 8,  4,  2],\n",
        "                     [16,  0,  1],\n",
        "                     [32, 64,128]],dtype=np.uint8)\n",
        "\n",
        "    # iterate either 1) indefinitely or 2) up to iteration limit\n",
        "    while n != 0:\n",
        "        before = np.sum(skel) # count points before thinning\n",
        "        \n",
        "        # for each subiteration\n",
        "        for lut in [G123_LUT, G123P_LUT]:\n",
        "            # correlate image with neighborhood mask\n",
        "            N = ndi.correlate(skel, mask, mode='constant')\n",
        "            # take deletion decision from this subiteration's LUT\n",
        "            D = np.take(lut, N)\n",
        "            # perform deletion\n",
        "            skel[D] = 0\n",
        "            \n",
        "        after = np.sum(skel) # coint points after thinning\n",
        "        \n",
        "        if before == after:\n",
        "            # iteration had no effect: finish\n",
        "            break\n",
        "            \n",
        "        # count down to iteration limit (or endlessly negative)\n",
        "        n -= 1\n",
        "    \n",
        "    return skel.astype(bool)\n",
        "\n",
        "\n",
        "def Fmeasure(predict_img_,GT_img_):\n",
        "    temp_tp = (1-predict_img_) * (1-GT_img_)\n",
        "    temp_fp = (1-predict_img_) * GT_img_\n",
        "    temp_fn = predict_img_ * (1-GT_img_)\n",
        "    temp_tn = predict_img_ * GT_img_\n",
        "    count_tp = sum(sum(temp_tp))\n",
        "    count_fp = sum(sum(temp_fp))\n",
        "    count_fn = sum(sum(temp_fn))\n",
        "    count_tn = sum(sum(temp_tn))\n",
        "    temp_p = count_tp / (count_fp + count_tp + 1e-4)\n",
        "    temp_r = count_tp / (count_fn + count_tp + 1e-4)\n",
        "    temp_f = 2 * (temp_p * temp_r) / (temp_p + temp_r + 1e-4)\n",
        "    return temp_f\n",
        "\n",
        "def Psnr(predict_img_,GT_img_):\n",
        "    temp_fp = (1-predict_img_) * GT_img_\n",
        "    temp_fn = predict_img_ * (1-GT_img_)\n",
        "    xm = GT_img_.shape[0]\n",
        "    ym = GT_img_.shape[1]\n",
        "    fp_fn = temp_fp + temp_fn\n",
        "    fp_fn[fp_fn>0] = 1\n",
        "    fp_fn[fp_fn==0] = 0\n",
        "    err=sum(sum(fp_fn)) / (xm * ym) \n",
        "    temp_PSNR = 10 * math.log( 1 / err,10)\n",
        "    return temp_PSNR\n",
        "\n",
        "def Pfmeasure(predict_img_,GT_img_):\n",
        "    N_GT_img_ = 1 - GT_img_\n",
        "    skel_GT = bwmorph_thin(N_GT_img_)\n",
        "    skel_GT = (skel_GT).astype('uint8')\n",
        "    skel_GT = 1 - skel_GT\n",
        "    temp_tp = (1-predict_img_) * (1-GT_img_)\n",
        "    temp_fp = (1-predict_img_) * GT_img_\n",
        "    temp_fn = predict_img_ * (1-GT_img_)\n",
        "    temp_tn = predict_img_ * GT_img_\n",
        "    count_tp = sum(sum(temp_tp))\n",
        "    count_fp = sum(sum(temp_fp))\n",
        "    count_fn = sum(sum(temp_fn))\n",
        "    count_tn = sum(sum(temp_tn))\n",
        "    temp_p = count_tp / (count_fp + count_tp + 1e-4) \n",
        "    temp_skl_tp = (1-predict_img_) * (1-skel_GT)\n",
        "    temp_skl_fp = (1-predict_img_) * skel_GT\n",
        "    temp_skl_fn = predict_img_ * (1-skel_GT)\n",
        "    temp_skl_tn = predict_img_ * skel_GT\n",
        "    count_skl_tp = sum(sum(temp_skl_tp))\n",
        "    count_skl_fp = sum(sum(temp_skl_fp))\n",
        "    count_skl_fn = sum(sum(temp_skl_fn))\n",
        "    count_skl_tn = sum(sum(temp_skl_tn))\n",
        "    temp_pseudo_p = count_skl_tp / (count_skl_fp + count_skl_tp + 1e-4) \n",
        "    temp_pseudo_r = count_skl_tp / (count_skl_fn + count_skl_tp + 1e-4) \n",
        "    temp_pseudo_f = 2 * (temp_p * temp_pseudo_r) / (temp_p + temp_pseudo_r + 1e-4)\n",
        "    return temp_pseudo_f\n",
        "\n",
        "\n",
        "def DRD(predict_img_,GT_img_):\n",
        "    xm = GT_img_.shape[0]\n",
        "    ym = GT_img_.shape[1]\n",
        "    blkSize=8 \n",
        "    MaskSize=5 \n",
        "    u0_GT1 = np.zeros((xm+2,ym+2)) \n",
        "    u0_GT1[1 : xm + 1, 1 : ym + 1] = GT_img_\n",
        "    intim = np.cumsum(np.cumsum(u0_GT1, 0), 1)\n",
        "    NUBN = 0\n",
        "    blkSizeSQR = blkSize * blkSize\n",
        "    counter = 0\n",
        "    for i in range(1,(xm - blkSize + 1),blkSize): \n",
        "        for j in range(1,(ym - blkSize + 1),blkSize): \n",
        "            \n",
        "            blkSum=intim[i + blkSize - 1, j + blkSize - 1] - intim[i - 1, j + blkSize - 1] - intim[i + blkSize - 1, j - 1] + intim[i - 1, j -1] \n",
        "            if blkSum == 0:\n",
        "                pass\n",
        "            elif blkSum == blkSizeSQR: \n",
        "                counter += 1;\n",
        "                pass\n",
        "            else: \n",
        "                NUBN = NUBN + 1\n",
        "    wm = np.zeros((MaskSize, MaskSize))\n",
        "    ic = int((MaskSize + 1) / 2 ) \n",
        "    jc = ic \n",
        "    for i in range(0,MaskSize): \n",
        "        for j in range(0,MaskSize): \n",
        "            num = math.sqrt((i+1 - ic) * (i+1 - ic) + (j+1 - jc) * (j+1 - jc))\n",
        "            if num == 0: \n",
        "                wm[i, j]=0\n",
        "            else: \n",
        "                wm[i, j] = 1 / num\n",
        "    wnm = wm / sum(sum(wm)) \n",
        "    u0_GT_Resized = np.zeros((xm + ic + 1, ym + jc + 1)) \n",
        "    u0_GT_Resized[ic-1 : xm + ic - 1, jc-1 : ym + jc - 1]= GT_img_\n",
        "    u_Resized = np.zeros((xm + ic + 1, ym + jc + 1)) \n",
        "    u_Resized[ic-1 : xm + ic - 1, jc-1 : ym + jc - 1] = predict_img_\n",
        "    temp_fp_Resized = (1-u_Resized) * u0_GT_Resized \n",
        "    temp_fn_Resized = u_Resized * (1-u0_GT_Resized) \n",
        "    Diff = temp_fp_Resized+temp_fn_Resized \n",
        "    Diff[Diff==0] = 0 \n",
        "    Diff[Diff>0] = 1 \n",
        "    xm2 = Diff.shape[0] \n",
        "    ym2 = Diff.shape[1] \n",
        "    SumDRDk = 0\n",
        "    def my_xor_infile(u_infile, u0_GT_infile): \n",
        "        temp_fp_infile = (1-u_infile) * u0_GT_infile \n",
        "        temp_fn_infile = u_infile * (1-u0_GT_infile) \n",
        "        temp_xor_infile = temp_fp_infile + temp_fn_infile \n",
        "        temp_xor_infile[temp_xor_infile==0] = 0 \n",
        "        temp_xor_infile[temp_xor_infile>0] = 1 \n",
        "        return temp_xor_infile\n",
        "    for i in range(ic-1,xm2 - ic + 1): \n",
        "        for j in range(jc-1,ym2 - jc + 1): \n",
        "            if Diff[i,j] == 1: \n",
        "                Local_Diff = my_xor_infile(u0_GT_Resized[i - ic +1 : i + ic  , j - ic+1 : j + ic ], u_Resized[i, j]) \n",
        "                DRDk = sum(sum(Local_Diff * wnm)) \n",
        "                SumDRDk = SumDRDk + DRDk       \n",
        "    temp_DRD = SumDRDk / (NUBN + 1e-4)\n",
        "    return temp_DRD"
      ],
      "metadata": {
        "id": "ekuwuxyWFX1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger = transformers.logging.get_logger()\n",
        "# metric = evaluate.load(FLAGS[\"metric\"])\n",
        "\n",
        "model_kwargs = {\n",
        "    \"id2label\": id2label, \n",
        "    \"label2id\": label2id,\n",
        "    \"ignore_mismatched_sizes\": True\n",
        "}\n",
        "\n",
        "model = transformers.SegformerForSemanticSegmentation.from_pretrained(\n",
        "    FLAGS[\"base_model_name\"], **model_kwargs\n",
        ")\n",
        "\n",
        "\n",
        "def compute_metrics(outputs: transformers.EvalPrediction) -> typing.Dict[str, float]:\n",
        "    with torch.no_grad():\n",
        "        logits, labels = outputs\n",
        "        logits = torch.from_numpy(logits)\n",
        "\n",
        "        # It upscales the logits to the size of the label\n",
        "        logits = nn.functional.interpolate(\n",
        "            logits,\n",
        "            size=labels.shape[-2:],\n",
        "            mode=\"bilinear\",\n",
        "            align_corners=False,\n",
        "        ).argmax(dim=1)\n",
        "\n",
        "        predictions = logits.detach().cpu().numpy()\n",
        "\n",
        "        batch_size, height, width = logits.shape\n",
        "\n",
        "        npixel = height*width\n",
        "\n",
        "        fmeasures = []\n",
        "        pfmeasures = []\n",
        "        psnrs = []\n",
        "        drds = []\n",
        "\n",
        "        for idx in range(batch_size):\n",
        "            im = predictions[idx]\n",
        "            im_gt = labels[idx]\n",
        "\n",
        "            fmeasure = Fmeasure(im, im_gt)\n",
        "            psnr = Psnr(im, im_gt)\n",
        "            pfmeasure = Pfmeasure(im, im_gt)\n",
        "            drd = DRD(im, im_gt)\n",
        "\n",
        "            fmeasures.append(fmeasure)\n",
        "            pfmeasures.append(pfmeasure)\n",
        "            psnrs.append(psnr)\n",
        "            drds.append(drd)\n",
        "\n",
        "        batch_fmeasure = np.mean(fmeasures)\n",
        "        batch_pfmeasure = np.mean(pfmeasures)\n",
        "        batch_psnr = np.mean(psnrs)\n",
        "        batch_drd = np.mean(drds)\n",
        "\n",
        "        metrics = {\n",
        "            \"fmeasure\": batch_fmeasure,\n",
        "            \"pfmeasure\": batch_pfmeasure,\n",
        "            \"psnr\": batch_psnr,\n",
        "            \"drd\": batch_drd\n",
        "        }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "        # # FIXME: For more information, see\n",
        "        # # https://github.com/huggingface/evaluate/pull/328#issuecomment-1286866576\n",
        "        # metrics = metric._compute(\n",
        "        #         predictions=predictions,\n",
        "        #         references=labels,\n",
        "        #         num_labels=num_labels,\n",
        "        #         ignore_index=0,  # The background info is ignored\n",
        "        #         reduce_labels=processor.do_reduce_labels,\n",
        "        #     )\n",
        "        \n",
        "        # # It adds per-category metrics as separate key-val pairs\n",
        "        # per_category_accuracy = metrics.pop(\"per_category_accuracy\").tolist()\n",
        "        # per_category_iou = metrics.pop(\"per_category_iou\").tolist()\n",
        "\n",
        "        # metrics.update({f\"accuracy_{id2label[key]}\": val for key, val in enumerate(per_category_accuracy)})\n",
        "        # metrics.update({f\"iou_{id2label[key]}\": val for key, val in enumerate(per_category_iou)})\n",
        "        \n",
        "        # return metrics\n",
        "\n",
        "\n",
        "training_args = transformers.TrainingArguments(\n",
        "    auto_find_batch_size=True,\n",
        "    eval_accumulation_steps=FLAGS[\"accumulation_steps\"],\n",
        "    eval_steps=10,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    fp16=False,\n",
        "    full_determinism=False,\n",
        "    gradient_accumulation_steps=FLAGS[\"accumulation_steps\"],\n",
        "    hub_model_id=FLAGS[\"model_name\"],\n",
        "    hub_strategy=\"end\",\n",
        "    learning_rate=FLAGS[\"learning_rate\"],\n",
        "    load_best_model_at_end=True,\n",
        "    logging_steps=5,\n",
        "    lr_scheduler_type=FLAGS[\"scheduler_type\"],\n",
        "    num_train_epochs=FLAGS[\"num_epochs\"],\n",
        "    optim=FLAGS[\"optimizer\"],\n",
        "    output_dir=FLAGS[\"model_name\"],\n",
        "    per_device_eval_batch_size=FLAGS[\"batch_size\"],\n",
        "    per_device_train_batch_size=FLAGS[\"batch_size\"],\n",
        "    push_to_hub=True,\n",
        "    remove_unused_columns=False,  # https://discuss.huggingface.co/t/divide-by-zero-error-when-following-ch7-tutorial/18393/6\n",
        "    report_to=\"tensorboard\",\n",
        "    save_steps=10,\n",
        "    save_strategy=\"steps\",\n",
        "    save_total_limit=3,\n",
        "    seed=10,\n",
        "    warmup_steps=50,\n",
        ")\n",
        "\n",
        "# decay_parameters = get_parameter_names(model, [nn.LayerNorm])\n",
        "# decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n",
        "\n",
        "# c = [\n",
        "#     {\n",
        "#         \"params\": [\n",
        "#              param for name, param in model.named_parameters() \n",
        "#              if name in decay_parameters\n",
        "#         ],\n",
        "#         \"weight_decay\": training_args.weight_decay,\n",
        "#     },\n",
        "#     {\n",
        "#         \"params\": [\n",
        "#              param for name, param in model.named_parameters() \n",
        "#              if name in decay_parameters\n",
        "#         ],\n",
        "#         \"weight_decay\": 0.0,\n",
        "#     },\n",
        "# ]\n",
        "\n",
        "# optim_kwargs = {\n",
        "#     \"betas\": (training_args.adam_beta1, training_args.adam_beta2),\n",
        "#     \"eps\": training_args.adam_epsilon,\n",
        "# }\n",
        "# optim_kwargs[\"lr\"] = training_args.learning_rate\n",
        "\n",
        "# adam_8bit_optim = bitsandbytes.optim.Adam8bit(\n",
        "#     optim_kwargs,\n",
        "#     betas=(training_args.adam_beta1, training_args.adam_beta2),\n",
        "#     eps=training_args.adam_epsilon,\n",
        "#     lr=training_args.learning_rate,\n",
        "# )\n",
        "\n",
        "callbacks = [\n",
        "    transformers.EarlyStoppingCallback(early_stopping_patience=5)\n",
        "]\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    args=training_args,\n",
        "    callbacks=callbacks,\n",
        "    compute_metrics=compute_metrics,\n",
        "    eval_dataset=test_dataset,\n",
        "    model=model,    \n",
        "    train_dataset=train_dataset,\n",
        "    # optimizers=(adam_8bit_optim, None)\n",
        ")\n",
        "\n",
        "try:\n",
        "    checkpoint = get_last_checkpoint(FLAGS[\"model_name\"])\n",
        "except FileNotFoundError:\n",
        "    logger.debug(\"No checkpoint\")\n",
        "    checkpoint = None\n",
        "\n",
        "resume_from_checkpoint = checkpoint is not None\n",
        "\n",
        "trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
        "\n",
        "kwargs = {\n",
        "    \"finetuned_from\": FLAGS[\"base_model_name\"],\n",
        "    \"tags\": [\n",
        "        \"document-image-binarization\"\n",
        "        \"image-segmentation\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "trainer.push_to_hub(**kwargs)"
      ],
      "metadata": {
        "id": "8foaMGZlLEzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Inference\n",
        "\n",
        "For a complete example, see T. Cornille's official Segformer [blog post](https://huggingface.co/blog/fine-tune-segformer)"
      ],
      "metadata": {
        "id": "w2RW-caNNh4T"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nTgfBrTw25u1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}